{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c3518d6",
   "metadata": {},
   "source": [
    "# Unzip bbox files in Azure Blob Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c70f2ba9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'azure'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13487/3132835034.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mazure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblob\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBlobClient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtarfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcompute_data_folder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"../../data/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'azure'"
     ]
    }
   ],
   "source": [
    "from azure.storage.blob import BlobClient\n",
    "import tarfile\n",
    "import os\n",
    "\n",
    "compute_data_folder = \"../../data/\"\n",
    "zip_files = [\"Segments_mat_gt_bb_S11.tgz\", \n",
    "             \"Segments_mat_gt_bb_S5.tgz\", \"Segments_mat_gt_bb_S6.tgz\", \"Segments_mat_gt_bb_S7.tgz\",\n",
    "            \"Segments_mat_gt_bb_S8.tgz\", \"Segments_mat_gt_bb_S9.tgz\"]\n",
    "#zip_files = [\"Segments_mat_gt_bb_S1.tgz\"]\n",
    "conn_str = \"DefaultEndpointsProtocol=https;AccountName=nyposeestimation;AccountKey=Uf7Ket/F5XqlnHJx3vbJ/HDJNNzGpnUIuNo2rCg4CxAVHgF3+V574b2Siw6EM0Ef5+Rw5aWUd2k++AStXDZ8fQ==;EndpointSuffix=core.windows.net\"\n",
    "container_name = \"human36m\"\n",
    "blob_data_folder = \"data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26589471",
   "metadata": {
    "gather": {
     "logged": 1670784748011
    }
   },
   "outputs": [],
   "source": [
    "for zip_file in zip_files:\n",
    "    # Get blob client\n",
    "    blob_client = BlobClient.from_connection_string(conn_str=conn_str, \n",
    "                                             container_name=container_name, \n",
    "                                             blob_name=blob_data_folder + zip_file)\n",
    "    # Save file into compute instance\n",
    "    print(\"File: \", zip_file)\n",
    "    print(\"Starting to save zip file into compute instance...\")\n",
    "    with open(compute_data_folder + zip_file, \"wb+\") as blob:\n",
    "        blob_data = blob_client.download_blob()\n",
    "        blob_data.readinto(blob)\n",
    "    print(\"Finished saving zip file into compute instance.\")\n",
    "    \n",
    "    # Unzip file in compute instance\n",
    "    print(\"Starting to unzip file in compute instance...\")\n",
    "    unzipped_folder = zip_file[0:-4]\n",
    "    unzipped_folder_prefix = compute_data_folder + unzipped_folder\n",
    "    file = tarfile.open(compute_data_folder + zip_file)\n",
    "    file.extractall(unzipped_folder_prefix)\n",
    "    file.close()\n",
    "    print(\"Finished unzipping file in compute instance.\")\n",
    "    \n",
    "    # Upload bboxes back to azure storage\n",
    "    print(\"Getting list of bboxes mat files\")\n",
    "    bboxes = os.listdir(unzipped_folder_prefix + \"/\" + unzipped_folder[19:] + \"/MySegmentsMat/ground_truth_bb\")\n",
    "    # print(bboxes) # for double checking that only the first two are extraneous files, or not\n",
    "    \n",
    "    print(\"Starting to upload individual bboxes back to azure...\")\n",
    "        # For efficiency, store these path prefixes\n",
    "    blob_new_folder = blob_data_folder + unzipped_folder + \"/\"\n",
    "    compute_video_prefix = unzipped_folder_prefix + \"/\" + unzipped_folder[19:] + \"/MySegmentsMat/ground_truth_bb/\"\n",
    "    for bbox in bboxes:\n",
    "        blob = BlobClient.from_connection_string(conn_str=conn_str, \n",
    "                                                 container_name=container_name, \n",
    "                                                 blob_name=blob_new_folder + bbox)\n",
    "        # blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_folder_prefix + zip_files[0][0:-4] + \"/\" + video)\n",
    "\n",
    "        \n",
    "        with open(file=compute_video_prefix + bbox, mode=\"rb\") as data:\n",
    "            blob.upload_blob(data)\n",
    "    print(\"Done uploading \", zip_file, \" to Azure.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6c2fff",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Utils\n",
    "\n",
    "### OpenCV Vocabulary\n",
    "TCHW: Time (t), Channel (RGB), H (Height), W (Width)\n",
    "\n",
    "### Directory Structure:\n",
    "\n",
    "---- Videos_S11/\n",
    "\n",
    "    ---- Videos/\n",
    "        ----[].mp4\n",
    "        ----[].mp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8eb9f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#import torch\n",
    "#import matplotlib.pyplot as plt\n",
    "#import torchvision.transforms.functional as F\n",
    "#from torchvision.io import read_video # extract frames\n",
    "import os\n",
    "import h5py\n",
    "#from torchvision.utils import save_image\n",
    "#import torchvision.transforms as T\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e5a6b2",
   "metadata": {},
   "source": [
    "## Post-process frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83e13375",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"plt.rcParams[\"savefig.bbox\"] = \"tight\"\n",
    "# sphinx_gallery_thumbnail_number = 2\n",
    "\n",
    "def plot(imgs, **imshow_kwargs):\n",
    "    if not isinstance(imgs[0], list):\n",
    "        # Make a 2d grid even if there's just 1 row\n",
    "        imgs = [imgs]\n",
    "\n",
    "    num_rows = len(imgs)\n",
    "    num_cols = len(imgs[0])\n",
    "    _, axs = plt.subplots(nrows=num_rows, ncols=num_cols, squeeze=False)\n",
    "    for row_idx, row in enumerate(imgs):\n",
    "        for col_idx, img in enumerate(row):\n",
    "            ax = axs[row_idx, col_idx]\n",
    "            img = F.to_pil_image(img.to(\"cpu\"))\n",
    "            ax.imshow(np.asarray(img), **imshow_kwargs)\n",
    "            ax.set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\"\"\"\n",
    "\n",
    "# get frames from video path\n",
    "def get_frames(video_path):\n",
    "    frames, _, _ = read_video(video_path, output_format=\"TCHW\", pts_unit=\"sec\")\n",
    "    return frames\n",
    "\n",
    "# make frame suitable for inference\n",
    "def make_frame_suitable(frame):\n",
    "    transform = T.ToPILImage()\n",
    "    frame = transform(frame)\n",
    "    frame = frame.resize((256, 256)) # resize to 256 x 256\n",
    "    return frame\n",
    "\n",
    "def frame_resize_to_nparr(frame):\n",
    "    transform = T.ToPILImage()\n",
    "    frame = transform(frame)\n",
    "    frame = frame.resize((256, 256)) # resize to 256 x 256\n",
    "    return np.array(frame)\n",
    "\n",
    "def frame_to_tensor(frame):\n",
    "    transform = T.ToPILImage()\n",
    "    frame = transform(frame)\n",
    "    frame = frame.resize((256, 256)) # resize to 256 x 256\n",
    "    toTensor = T.ToTensor()\n",
    "    return toTensor(frame)\n",
    "\n",
    "# get list of all subjects\n",
    "def get_subjects():\n",
    "    video_folders = os.listdir(compute_data_folder)[4:]\n",
    "    return video_folders\n",
    "\n",
    "def get_video_paths(subject):\n",
    "    video_paths = os.listdir(compute_data_folder + subject + \"/videos/\")[2:]\n",
    "    return video_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e66357",
   "metadata": {},
   "source": [
    "## Pre-process frames (tasks split to increase efficiency)\n",
    "\n",
    "h36m\n",
    "\n",
    "    +-- training\n",
    "    \n",
    "        +-- subject\n",
    "        \n",
    "            +-- frames\n",
    "            \n",
    "                +-- activity\n",
    "                \n",
    "                    +-- frame*.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a52c3599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing subject  S1\n",
      "Processing subject  S11\n",
      "Processing subject  S5\n",
      "Processing subject  S6\n",
      "Processing subject  S7\n",
      "Processing subject  S8\n",
      "Processing subject  S9\n"
     ]
    }
   ],
   "source": [
    "# CREATE DIRECTORY STRUCTURE\n",
    "subjects = get_subjects()\n",
    "\n",
    "# for each subject\n",
    "for subject in subjects:\n",
    "    video_paths = get_video_paths(subject)\n",
    "    path_prefix = compute_data_folder + subject\n",
    "    \n",
    "    save_prefix = compute_data_folder + \"h36m_final/training/\" + subject + \"/frames/\"\n",
    "    \n",
    "    print(\"Processing subject \", subject)\n",
    "    \n",
    "    # for each video (i.e. activity), get list of frames\n",
    "    for j in range(len(video_paths)):\n",
    "        # print(\"----video \", (j+1) / len(video_paths))\n",
    "        \n",
    "        # get activity\n",
    "        activity = video_paths[j][:-4]\n",
    "        \n",
    "        # path prefix for video\n",
    "        save_prefix_vid = save_prefix + activity\n",
    "        \n",
    "        if not os.path.isdir(save_prefix_vid): # create directory structure is not present\n",
    "            os.makedirs(save_prefix_vid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d580f2cc-6bb7-4539-a8a0-197b52dcd7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = get_subjects()\n",
    "subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8baf942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['SittingDown.58860488.mp4',\n",
       " 'SittingDown.60457274.mp4',\n",
       " 'Smoking 1.54138969.mp4',\n",
       " 'Smoking 1.55011271.mp4',\n",
       " 'Smoking 1.58860488.mp4',\n",
       " 'Smoking 1.60457274.mp4',\n",
       " 'Smoking.54138969.mp4',\n",
       " 'Smoking.55011271.mp4',\n",
       " 'Smoking.58860488.mp4',\n",
       " 'Smoking.60457274.mp4',\n",
       " 'TakingPhoto 1.54138969.mp4',\n",
       " 'TakingPhoto 1.55011271.mp4',\n",
       " 'TakingPhoto 1.58860488.mp4',\n",
       " 'TakingPhoto 1.60457274.mp4',\n",
       " 'TakingPhoto.54138969.mp4',\n",
       " 'TakingPhoto.55011271.mp4',\n",
       " 'TakingPhoto.58860488.mp4',\n",
       " 'TakingPhoto.60457274.mp4',\n",
       " 'Waiting 1.54138969.mp4',\n",
       " 'Waiting 1.55011271.mp4',\n",
       " 'Waiting 1.58860488.mp4',\n",
       " 'Waiting 1.60457274.mp4',\n",
       " 'Waiting.54138969.mp4',\n",
       " 'Waiting.55011271.mp4',\n",
       " 'Waiting.58860488.mp4',\n",
       " 'Waiting.60457274.mp4',\n",
       " 'Walking 1.54138969.mp4',\n",
       " 'Walking 1.55011271.mp4',\n",
       " 'Walking 1.58860488.mp4',\n",
       " 'Walking 1.60457274.mp4',\n",
       " 'Walking.54138969.mp4',\n",
       " 'Walking.55011271.mp4',\n",
       " 'Walking.58860488.mp4',\n",
       " 'Walking.60457274.mp4',\n",
       " 'WalkingDog 1.54138969.mp4',\n",
       " 'WalkingDog 1.55011271.mp4',\n",
       " 'WalkingDog 1.58860488.mp4',\n",
       " 'WalkingDog 1.60457274.mp4',\n",
       " 'WalkingDog.54138969.mp4',\n",
       " 'WalkingDog.55011271.mp4',\n",
       " 'WalkingDog.58860488.mp4',\n",
       " 'WalkingDog.60457274.mp4',\n",
       " 'WalkTogether 1.54138969.mp4',\n",
       " 'WalkTogether 1.55011271.mp4',\n",
       " 'WalkTogether 1.58860488.mp4',\n",
       " 'WalkTogether 1.60457274.mp4',\n",
       " 'WalkTogether.54138969.mp4',\n",
       " 'WalkTogether.55011271.mp4',\n",
       " 'WalkTogether.58860488.mp4',\n",
       " 'WalkTogether.60457274.mp4',\n",
       " '_ALL 1.54138969.mp4',\n",
       " '_ALL 1.55011271.mp4',\n",
       " '_ALL 1.58860488.mp4',\n",
       " '_ALL 1.60457274.mp4',\n",
       " '_ALL.54138969.mp4',\n",
       " '_ALL.55011271.mp4',\n",
       " '_ALL.58860488.mp4',\n",
       " '_ALL.60457274.mp4']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_paths = get_video_paths(\"S1\")\n",
    "for i in range(len(video_paths)):\n",
    "    if video_paths[i] == \"SittingDown.55011271.mp4\":\n",
    "        print(i)\n",
    "        break\n",
    "video_paths[70:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e8170e1",
   "metadata": {
    "gather": {
     "logged": 1673491470887
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing subject  S1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_26462/1224805513.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframe_resize_to_nparr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_prefix_vid\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"frame%04d.png\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;31m# save frame as JPEG file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "subjects = get_subjects()\n",
    "#subjects = ['S1', 'S11', 'S5']\n",
    "\n",
    "# for each subject\n",
    "for subject in subjects:\n",
    "    video_paths = get_video_paths(subject)\n",
    "    path_prefix = compute_data_folder + subject\n",
    "    \n",
    "    save_prefix = compute_data_folder + \"h36m/training/\" + subject + \"/frames/\"\n",
    "    \n",
    "    print(\"Processing subject \", subject)\n",
    "    \n",
    "    # for each video (i.e. activity), get list of frames\n",
    "    \n",
    "    for j in range(len(video_paths)):\n",
    "        #if j % 3 == 0:\n",
    "        #    print(\"----video \", (j+1) / len(video_paths))\n",
    "        \n",
    "        video_path = path_prefix + \"/videos/\" + video_paths[j]\n",
    "        activity = video_paths[j][:-4]\n",
    "        \n",
    "        save_prefix_vid = save_prefix + activity + \"/\"\n",
    "        \n",
    "        # get frames for this video\n",
    "        vidcap = cv2.VideoCapture(video_path)\n",
    "        i = 0\n",
    "        while True:\n",
    "            success,image = vidcap.read()\n",
    "            if not success:\n",
    "                break\n",
    "            image = frame_resize_to_nparr(image)\n",
    "            cv2.imwrite(save_prefix_vid + (\"frame%04d.png\" % i), image)     # save frame as JPEG file\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521e7793",
   "metadata": {},
   "source": [
    "## Method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebae550e-da39-4bc8-a35e-0c99aeecebb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process bounding boxes of one video\n",
    "def get_masks_of_video(bbox_path):\n",
    "    with h5py.File(bbox_path, 'r') as f:\n",
    "        masks = np.array(f.get(\"Masks\"))\n",
    "        refs = np.array(f.get(\"#refs#\"))\n",
    "        \n",
    "        return masks[:,0]\n",
    "        \n",
    "        #print(refs)\n",
    "        \"\"\" thismask = masks[:,0][0]\n",
    "        frame = np.array(f[thismask])\n",
    "        print(frame.shape)\"\"\"\n",
    "        #np.savetxt(\"mask_example.csv\", frame, delimiter = \",\")\n",
    "\n",
    "        # for each frame\n",
    "        \"\"\"for mask in masks[:,0]:\n",
    "            frame = np.array(f[mask])\n",
    "            print(frame)\"\"\"\n",
    "        #print()\n",
    "        #print(refs.shape)\n",
    "        \n",
    "def get_masked_frame(bbox_path, mask):\n",
    "    with h5py.File(bbox_path, 'r') as f:\n",
    "        return np.array(f[mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b2a203-bfa7-49bd-8f85-405c60371b73",
   "metadata": {
    "gather": {
     "logged": 1672970594143
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "subjects = get_subjects()\n",
    "subjects = [\"S1\"]\n",
    "\n",
    "# for each subject\n",
    "for subject in subjects:\n",
    "    video_paths = get_video_paths(subject)\n",
    "    bbox_paths = [(i[:-4] + \".mat\") for i in video_paths]\n",
    "    \n",
    "    path_prefix = compute_data_folder + subject\n",
    "    # for each video, get list of frames and corresponding masks\n",
    "    for j in range(len(video_paths)):\n",
    "        video_path = path_prefix + \"/videos/\" + video_paths[j]\n",
    "        bbox_path = path_prefix + \"/bboxes/\" + bbox_paths[j]\n",
    "        frames = get_frames(video_path)\n",
    "        masks = get_masks_of_video(bbox_path)\n",
    "        \n",
    "        print(\"# Frames: \", len(frames))\n",
    "        print(\"# Masks: \", len(masks))\n",
    "        \n",
    "        pass\n",
    "        # for each frame\n",
    "        for k in range(len(masks)):\n",
    "            \n",
    "            # remove this later\n",
    "            # if (k % 100 == 0):\n",
    "            mask = masks[k]\n",
    "            frame = frames[k]\n",
    "            masked_frame = get_masked_frame(bbox_path, mask).T\n",
    "            #print(\"Frame shape: \", frame.shape)\n",
    "            #print(\"mask shape: \", masked_frame.shape)\n",
    "\n",
    "            masked_frame = frame * masked_frame\n",
    "            plot(torch.stack([masked_frame]))\n",
    "            \n",
    "        # find frames whose bounding box frames are in the same place\n",
    "            \n",
    "\n",
    "    #frames = get_frames(video_path)\n",
    "    #process_bbox(compute_data_folder + subject + \"/bboxes/\" + bbox_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b604caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get frames of one video\n",
    "video_path = \"../../data/S9/videos/Sitting 1.58860488.mp4\"\n",
    "frames = get_frames(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e99685",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(frames[25].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a745b5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "img1_batch = torch.stack([frames[0], frames[10], frames[20]])\n",
    "img2_batch = torch.stack([frames[20], frames[25]])\n",
    "\n",
    "plot(torch.stack([frames[1100]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715b77b4",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282e3dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pretrained_model import Predictor\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe52129-b792-4608-b6e3-3da8a4fc81e3",
   "metadata": {
    "gather": {
     "logged": 1670465770262
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')\n",
    "pred = Predictor(batch_size=1, num_parts=18, device=device, template_path='../../template.json',\n",
    "                 anchors_path='../../anchor_points.json')\n",
    "pred.load_checkpoint('../../checkpoint.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887462da-5a7c-4bdc-973b-c53299551280",
   "metadata": {
    "gather": {
     "logged": 1672970594500
    }
   },
   "outputs": [],
   "source": [
    "# choose one frame\n",
    "frame_example = frames[1100]\n",
    "frame_example = make_frame_suitable(frame_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cae19c8-9eff-401b-84d7-7cf375f3c8ab",
   "metadata": {
    "gather": {
     "logged": 1670465779173
    }
   },
   "outputs": [],
   "source": [
    "prediction = pred.predict(frame_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec50a3e-8f4f-4fe5-aa35-910a9d35bedc",
   "metadata": {},
   "source": [
    "# Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbd68d8-5d1f-43d4-9e54-af1a2da5cef1",
   "metadata": {
    "gather": {
     "logged": 1670465782581
    }
   },
   "outputs": [],
   "source": [
    "transformed_anchors = [0,0,0]\n",
    "warped_heatmaps, transformed_anchors[0], transformed_anchors[1], transformed_anchors[2] = prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a111ea0-e002-4a44-b5b2-b31a457ab40b",
   "metadata": {
    "gather": {
     "logged": 1670465783018
    }
   },
   "outputs": [],
   "source": [
    "warped_heatmaps_cpu = warped_heatmaps.squeeze().cpu()\n",
    "warped_heatmaps_cpu_numpy = warped_heatmaps_cpu.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fdd709-2902-4f39-b78a-572f55e63a6d",
   "metadata": {
    "gather": {
     "logged": 1670466127787
    }
   },
   "outputs": [],
   "source": [
    "warped_heatmaps_cpu_numpy = warped_heatmaps_cpu.detach().numpy()\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "warped_heatmaps_cpu_numpy_sum = np.sum(warped_heatmaps_cpu_numpy, 0)\n",
    "\n",
    "plt.imshow(warped_heatmaps_cpu_numpy_sum)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed457c1",
   "metadata": {},
   "source": [
    "## Show Original Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aee31df-2cf3-4899-bf13-5c3a1eb25eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_nums = pred.template.squeeze(0).cpu().detach().numpy()\n",
    "templates_sum = np.sum(template_nums, 0)\n",
    "\n",
    "plt.imshow(templates_sum)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65d2e72-55d2-4a5a-b306-66758bccd39b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "poseestimation"
  },
  "kernelspec": {
   "display_name": "Python (poseestimation)",
   "language": "python",
   "name": "poseestimation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
