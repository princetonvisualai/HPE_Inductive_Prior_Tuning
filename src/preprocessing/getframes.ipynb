{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from azure.storage.blob import BlobClient\n",
        "import tarfile\n",
        "import os\n",
        "\n",
        "compute_data_folder = \"../../data/\"\n",
        "zip_files = [\"Segments_mat_gt_bb_S11.tgz\", \n",
        "             \"Segments_mat_gt_bb_S5.tgz\", \"Segments_mat_gt_bb_S6.tgz\", \"Segments_mat_gt_bb_S7.tgz\",\n",
        "            \"Segments_mat_gt_bb_S8.tgz\", \"Segments_mat_gt_bb_S9.tgz\"]\n",
        "#zip_files = [\"Segments_mat_gt_bb_S1.tgz\"]\n",
        "conn_str = \"DefaultEndpointsProtocol=https;AccountName=nyposeestimation;AccountKey=Uf7Ket/F5XqlnHJx3vbJ/HDJNNzGpnUIuNo2rCg4CxAVHgF3+V574b2Siw6EM0Ef5+Rw5aWUd2k++AStXDZ8fQ==;EndpointSuffix=core.windows.net\"\n",
        "container_name = \"human36m\"\n",
        "blob_data_folder = \"data/\""
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {},
      "id": "c70f2ba9"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.transforms.functional as F\n",
        "from torchvision.io import read_video # extract frames\n",
        "import os\n",
        "import h5py\n",
        "from torchvision.utils import save_image\n",
        "import torchvision.transforms as T\n",
        "import cv2"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {},
      "id": "f8eb9f04"
    },
    {
      "cell_type": "code",
      "source": [
        "plt.rcParams[\"savefig.bbox\"] = \"tight\"\n",
        "# sphinx_gallery_thumbnail_number = 2\n",
        "\n",
        "def plot(imgs, **imshow_kwargs):\n",
        "    if not isinstance(imgs[0], list):\n",
        "        # Make a 2d grid even if there's just 1 row\n",
        "        imgs = [imgs]\n",
        "\n",
        "    num_rows = len(imgs)\n",
        "    num_cols = len(imgs[0])\n",
        "    _, axs = plt.subplots(nrows=num_rows, ncols=num_cols, squeeze=False)\n",
        "    for row_idx, row in enumerate(imgs):\n",
        "        for col_idx, img in enumerate(row):\n",
        "            ax = axs[row_idx, col_idx]\n",
        "            img = F.to_pil_image(img.to(\"cpu\"))\n",
        "            ax.imshow(np.asarray(img), **imshow_kwargs)\n",
        "            ax.set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# get frames from video path\n",
        "def get_frames(video_path):\n",
        "    frames, _, _ = read_video(video_path, output_format=\"TCHW\", pts_unit=\"sec\")\n",
        "    return frames\n",
        "\n",
        "# make frame suitable for inference\n",
        "def make_frame_suitable(frame):\n",
        "    transform = T.ToPILImage()\n",
        "    frame = transform(frame)\n",
        "    frame = frame.resize((256, 256)) # resize to 256 x 256\n",
        "    return frame\n",
        "\n",
        "def frame_resize_to_nparr(frame):\n",
        "    transform = T.ToPILImage()\n",
        "    frame = transform(frame)\n",
        "    frame = frame.resize((256, 256)) # resize to 256 x 256\n",
        "    return np.array(frame)\n",
        "\n",
        "def frame_to_tensor(frame):\n",
        "    transform = T.ToPILImage()\n",
        "    frame = transform(frame)\n",
        "    frame = frame.resize((256, 256)) # resize to 256 x 256\n",
        "    toTensor = T.ToTensor()\n",
        "    return toTensor(frame)\n",
        "\n",
        "# get list of all subjects\n",
        "def get_subjects():\n",
        "    video_folders = os.listdir(compute_data_folder)[3:]\n",
        "    return video_folders\n",
        "\n",
        "def get_video_paths(subject):\n",
        "    video_paths = os.listdir(compute_data_folder + subject + \"/videos/\")[2:]\n",
        "    return video_paths"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {},
      "id": "83e13375"
    },
    {
      "cell_type": "code",
      "source": [
        "subjects = get_subjects()\n",
        "#subjects = ['S1', 'S11', 'S5']\n",
        "\n",
        "# for each subject\n",
        "for subject in subjects:\n",
        "    video_paths = get_video_paths(subject)\n",
        "    path_prefix = compute_data_folder + subject\n",
        "    \n",
        "    save_prefix = compute_data_folder + \"h36m/training/\" + subject + \"/frames/\"\n",
        "    \n",
        "    print(\"Processing subject \", subject)\n",
        "    \n",
        "    # for each video (i.e. activity), get list of frames\n",
        "    \n",
        "    for j in range(len(video_paths)):\n",
        "        #if j % 3 == 0:\n",
        "        #    print(\"----video \", (j+1) / len(video_paths))\n",
        "        \n",
        "        video_path = path_prefix + \"/videos/\" + video_paths[j]\n",
        "        activity = video_paths[j][:-4]\n",
        "        \n",
        "        save_prefix_vid = save_prefix + activity + \"/\"\n",
        "        \n",
        "        # get frames for this video\n",
        "        vidcap = cv2.VideoCapture(video_path)\n",
        "        i = 0\n",
        "        while True:\n",
        "            success,image = vidcap.read()\n",
        "            if not success:\n",
        "                break\n",
        "            image = frame_resize_to_nparr(image)\n",
        "            cv2.imwrite(save_prefix_vid + (\"frame%04d.png\" % i), image)     # save frame as JPEG file\n",
        "            i += 1"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Processing subject  S1\n"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_26462/1224805513.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframe_resize_to_nparr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_prefix_vid\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"frame%04d.png\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;31m# save frame as JPEG file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1673314000901
        }
      },
      "id": "9e8170e1"
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "poseestimation"
    },
    "kernelspec": {
      "name": "poseestimation",
      "language": "python",
      "display_name": "Python (poseestimation)"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.15",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}